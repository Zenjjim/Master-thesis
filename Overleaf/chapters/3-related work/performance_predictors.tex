\subsection{How Powerful are Performance Predictors in Neural Architecture Search}

\cite{white2021powerful} conducted a extensive study of performance predictors for \gls{NAS}. 31 different predictors were utilised within the experiments across four search spaces and four datasets (NAS-Bench-201 with CIFAR-10, Cifar-100 and ImageNet16-120, NAS-Bench-101 and DARTS with CIFAR-10, and NAS-Bench-NLP with Penn TreeBank). For each predictor, the evaluation consisted of the initialisation time, query time and performance.

The study's motivation was that there were no existing approaches in the literature comparing the different performance predictors to each other. For each performance predictor, one had to go to the original paper proposing the method to find the evaluation. Consequently, the authors wanted to determine how zero-cost, model-based, learning curve extrapolation and weight-sharing methods compared. 

Further, the paper proposed a new predictor, OMNI, which combines complementary information from three different families (learning curve, zero-cost and model-based) of performance predictors. OMNI showed great promise with substantially improved performance. 

\subsection{Neural Architecture Search without Training}
\cite{mellor2021neural} argued that \gls{NAS} algorithms tend to be slow due to their extensive need for training architectures to guide the search algorithm. As a result, a proposed method for estimating a neural network's trained performance without needing to train it was developed. So, the authors show that it is, in fact, possible to perform \gls{NAS} without training any of the architectures. The paper shows that by utilising the method, a network which achieves 92.81\% accuracy was obtained within 30 seconds within the NAS-Bench-201 search space - remarkably faster than standard \gls{NAS} algorithms. The results sparked the interest in scoring neural networks at initialisation and inspired other authors to create similar metrics. 

