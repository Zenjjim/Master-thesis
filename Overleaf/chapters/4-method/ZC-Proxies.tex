\section{Zero-Cost Proxies}\label{zcproxies}

This section aims to provide a comprehensive overview of the different zero-cost proxies implemented in the project and clarify their respective methodologies.


\subsection{EPE-NAS}
The main idea behind EPE-NAS is to assess how the network's gradients behave concerning the neural network's input. Thus, the need for training an entire network is eliminated \autocite{lopes2021epe}. 

A linear map is defined as $w_i = f(x_i)$, in which $x_i$ is a sample from a batch $X$. The linear map can be computed by \cref{eq:linear}. 

\begin{equation}
    J_i = \frac{\delta f(x_i)}{\delta x_i}
    \label{eq:linear}
\end{equation}

However, it is necessary to see how the network will behave through different data points from the dataset. Therefore, the Jacobian matrix $w_i$ for different data points, $f(x_i)$, is calculated:

\begin{equation}
J = \begin{pmatrix}
\frac{\delta f(x_1)}{\delta x_1} & \frac{\delta f(x_2)}{\delta x_2} & \dots & \frac{\delta f(x_N)}{\delta x_N} 
 \end{pmatrix}^T
\end{equation}

The Jacobian matrix shows how the output of the network changes based on the input. After that, the correlation between data points of the same class is calculated to say how the untrained network can model complex functions \autocite{lopes2021epe}. The correlations are then used to calculate the covariance matrix for each class in the dataset:

\begin{equation}
    C_{J_c} = (J - M_{J_c})(J-M_{J_c})^t
\end{equation}
, where $M_J$ is:

\begin{equation}
    (M_{J_c})_{i,j} = \frac{1}{N} \sum_{n \in {1,...,N}} J_{i,n}
\end{equation}

Then, the correlation matrix is calculated for each class:

\begin{equation}
    C_{J_c} = \frac{(C_{J_c})_{i,j}}{\sqrt{(C_{J_c})_{i,j} * (C_{J_c})_{j,j}}}
\end{equation}


Each class is individually evaluated by summing the logarithm of the absolute values of the correlation matrix elements plus a small value $k = 1 \times 10^{-5}$.

\begin{equation}
    E_c = 
    \begin{cases}
        \sum^N_{i=1} \sum^N_{j=1}\ log(|(\sum_{J_c})_{i,j}| + K), & \text{if}\ C \leq \tau \\ \\ 
        \frac{\sum^N_{i=1} \sum^N_{j=1}\ log(|(\sum_{J_c})_{i,j}| + K)}{||\sum_{J_c}||}, & \text{otherwise}
    \end{cases}
\end{equation}

The network's score can be calculated using the evaluations of the correlation matrices above: 

\begin{equation}
    s = 
    \begin{cases}
        \sum_{t=1}^C |e_t|, & \text{if}\ C \leq \tau \\ \\ 
        \frac{\sum_{t=e}^C \sum_{j=i+1}^C |e_i - e_j|}{||e||}, & \text{otherwise}
    \end{cases}
\end{equation}

$e$ is the vector with the scores of the correlation matrices. For example, in the original paper \autocite{lopes2021epe}, they found empirically that $\tau = 100$. 


\subsection{Fisher}
Initially introduced as a pruning method in \autocite{DBLP:journals/corr/abs-1801-05787}, Fisher aims to remove feature maps or parameters that do not significantly contribute to the model's overall performance. This is achieved by eliminating activation channels and their corresponding parameters, estimated to have a negligible impact on the loss \autocite{abdelfattah2021zero}.

\cite{DBLP:journals/corr/abs-1801-05787} employed \cref{eq:fisher} to calculate the network score. 

\begin{equation}
    \centering
    S_z(z) = \left(\frac{\partial L}{\partial z}z\right)^2, S_n = \sum_{i=1}^M S_z(z_i)
    \label{eq:fisher}
\end{equation}

where $M$ is the length of the vectorised feature map, and  $S_z$ is the saliency per activation $z$. 

By mapping all the different layers of the network, the activations can be captured and used to score the network.  

\subsection{Flops}
% \glsreset{FLOPs}
As a zero-cost proxy, \Gls{FLOPs} is one of the simpler baselines as it simply goes through the model and calculates the number of floating point operations performed per second required to pass the input through the network. Therefore it could be considered a measure of the architecture's complexity \autocite{ning2021evaluating}. 

\subsection{Grad Norm}
The Gradient Norm is a straightforward zero-cost proxy that computes the sum of the Euclidean norms of gradients after conducting a single forward and backward pass using a minibatch of training data \autocite{abdelfattah2021zero}. Given a vector of gradients \(G = [g_1, g_2, ..., g_n]\), the gradient norm is calculated according to the formula presented in \cref{eq:gradnorm}.

\begin{equation}\label{eq:gradnorm}
    s = \sum_{i=1}^n \sqrt{g_i^2}
\end{equation}


\subsection{GradSign}
GradSign uses the network's initial gradients like many other zero-cost proxies do. However, the proxy's central concept uses $\Psi$(psi) to examine the optimisation landscape of various networks at the level of specific training samples. Thus, GradSign differs from most other gradient-based methods because of its theoretical insights. Under plausible assumptions, these theoretical results show that a network with denser sample-wise local optima has lower training and generalisation losses.

The calculation of $\Psi$ is often considered computationally infeasible for modern networks. To tackle this issue, the authors propose GradSign, which approximates $\Psi$. This method takes as input a mini-batch of sample-wise gradients that are evaluated at a randomly initialised point. Using this input, the method produces statistical evidence strongly associated with the well-trained predictive performance of the network, as measured by its accuracy on the entire dataset \autocite{zhang2021gradsign}. 


\subsection{Grasp}
\glsreset{GRASP}
\gls{GRASP} is one of the pruning-at-initialisation techniques. \cite{wang2020picking} argue that efficient training depends on preserving the gradient flow, from which the name gradient signal preservation comes. Therefore, \gls{GRASP} aims at finding the sub-networks as initialisation rather than after training. This is done by pruning the weights whose removal will cause a minor fall in the gradient norm after pruning \autocite{wang2020picking}. 


\subsection{Jacov}
The Jacov-score is a zero-cost proxy that estimates architecture performance by analysing the gradient correlation structure in a neural network's output space \autocite{jacob_conv}. The Jacov-score is computed using the eigenvalues of the correlation matrix of the batch Jacobian, as shown in \cref{eq:jacov}. 

\begin{equation}
    s =-\sum_i \left( \log(v_i+k)+\frac{1}{v_i+k}\right)
\label{eq:jacov}
\end{equation}

where $k$ is a small constant. The Jacov-score offers an efficient performance evaluation in \gls{NAS} without extensive computation.

\subsection{L2-norm}
L2-norm is calculated by summing over the norm of all weights of each layer in a neural network. 


\subsection{NAS-WOT}

\cite{jacob_conv} derived a metric by investigating linear maps of binary activation codes from the overlapping ReLU present at initialisation. These linear maps provide insight into how the network divides the input space.

The intuition is that the more similar the binary codes associated with two inputs are, the more challenging it is for the network to learn to separate them. When two inputs have the same binary code, they lie within the same linear region of the network and are particularly difficult to disentangle \autocite{jacob_conv}.

Let's consider a mini-batch of data $X = \{x_i\}^N_{i=1}$ mapped through a neural network as $f(x_ii)$. We can define an indicator variable $c_i$, forming a binary code defining a linear region. Using the binary codes, we can use Hamming distance $d_H(c_i, c_j)$ to measure how different the two inputs are \autocite{jacob_conv}.

The correspondence between binary codes in the mini-batch can be computed with the kernel matrix, as given in \cref{eq:naswot}.

\begin{equation} 
K_H = \begin{bmatrix} 
        N_A-d_H(c_1, c_1) & \dots & N_A-d_H(c_1, c_N) \\ 
        \vdots & \ddots & \vdots \\ 
        N_A-d_H(c_N, c_1) & \dots & N_A-d_H(c_N, c_N)
    \end{bmatrix}, 
    \label{eq:naswot}
\end{equation}
where $N_A$ is the number of ReLU activations in the network. Then the final score metric $s$ is derived from the logarithm of the kernel norm at initialisation. 

\begin{equation}
s = |log|K_H||
\end{equation}


Because of matrix instability, a small epsilon was added to the diagonal to make it possible to calculate a determinant.


\subsection{Params}
Within a neural network, there exist several trainable parameters, which, similarly to \gls{FLOPs}, are considered a measure of the architecture's complexity \autocite{ning2021evaluating}. 

\subsection{Plain}
The Plain-score is simply looping through all layers of the network. If the layer's weight contains a gradient, the gradient is multiplied by the layer's weights. Ultimately, the method is summing the scores to obtain the final score. 

\subsection{Snip}
\glsreset{SNIP}
The \gls{SNIP} method was introduced by \autocite{lee2018snip} as a pruning-at-initialisation technique, aiming to reduce the number of parameters within a neural network without affecting the accuracy during inference \autocite{frankle2020pruning}. \cite{frankle2020pruning} proposed a data-dependent saliency criterion that identifies significant connections in the network related to the current task before training. This approach enables the pruning of redundant connections, thus reducing the neural network's parameter count. The connections are identified based on their influence on the loss function.

\noindent In the paper Zero-Cost Proxies for Lightweight \gls{NAS} \autocite{abdelfattah2021zero}, \gls{SNIP} was repurposed as a zero-cost proxy. The score of the neural network was obtained by summing all parameters $N$ in the model according to \cref{eq:sumsnip},

\begin{equation}\label{eq:sumsnip}
S_n = \sum_{i=1}^N S_p(\theta)_i
\end{equation}

\begin{equation}
S_p(\theta) = \left|\frac{\partial L}{\partial \theta} \odot \theta\right|
\end{equation}

where $L$ is the loss function, \(\theta \) represents the parameters of the neural network, and \(\odot\) denotes the Hadamard product.



\subsection{Synflow}
\glsreset{Synflow}
\gls{Synflow}, originally proposed in \autocite{tanaka2020pruning}, is a method for parameter pruning. By pruning less significant parameters, highly sparse networks can be obtained with reduced network size while maintaining similar accuracy. \gls{Synflow} is built on three main principles: avoiding layer collapse, conservation of synaptic saliency, and magnitude pruning.

\noindent Layer collapse occurs when an algorithm prunes all parameters in a single weight layer, even when prunable parameters remain elsewhere in the network. This makes the network untrainable, as evidenced by sudden drops in achievable accuracy \autocite{tanaka2020pruning}. Therefore, it is crucial to avoid layer collapse since the network will become unusable if a layer is removed.

\noindent Synaptic saliency is a class of score metrics that can be expressed as the Hadamard product (\cref{eq:hadamardproduct}).

\begin{equation}
S(\theta) = \frac{\partial R}{\partial \theta} \odot \theta
\label{eq:hadamardproduct}
\end{equation}
where $R$ is a scalar loss function of the output $y$ of a feed-forward network parameterized by $\theta$ \autocite{tanaka2020pruning}. The conservation of synaptic saliency theorem demonstrates that the sum of synaptic saliency for all incoming parameters to a hidden neuron equals the sum of the synaptic saliency for the outgoing parameters from the hidden neuron \autocite{tanaka2020pruning}. Consequently, the sum of synaptic saliency for all incoming parameters to a hidden layer equals the sum of the synaptic saliency for the outgoing parameters from the hidden layer. This can lead to issues when performing one-shot pruning, where a larger layer would have parameters with lower synaptic saliency. In comparison, a smaller layer would have parameters with higher synaptic saliency. Pruning one of the parameters for a smaller layer would result in pruning almost all the parameters in a larger layer to compensate for the conservation of synaptic saliency, leading to layer collapse.

\noindent To prevent this, \gls{Synflow} introduces iterative pruning (magnitude pruning) by recalculating the synaptic saliency for all parameters at each iteration. This re-evaluation process gives new scores to parameters with lower scores in previous iterations, eliminating the imbalance between smaller and larger layers and reducing the possibility of layer collapse.
\noindent The \gls{Synflow} algorithm is described in \cref{eq:synflowalg}:

\begin{equation}\label{eq:synflowalg}
R_{SF} = \mathbbm{1}^T \left( \prod^L_{l=1} |\theta^{[l]}| \right) \mathbbm{1}
\end{equation}

The algorithm takes an input of ones (e.g., an image where all the pixels have the value 1) and performs a forward pass through the network. The loss $R$ is obtained by calculating the product of the output. This loss is then backpropagated through the network to the layers, and \cref{eq:hadamardproduct} is used to compute the synaptic saliency score for each parameter $\theta$.


\subsection{Zen-score}
Zen-score is a measure of the expressivity of the network, which is computationally efficient and correlates positively with the model accuracy \autocite{lin2021zen}. Zen-score builds upon the expected Gaussian complexity ($\Phi$-score), which can define the expressivity of a \Gls{VCNN}. In theoretical investigations, \gls{VCNN} is a frequently used prototype in which several convolutional layers are stacked to form the main body of the network. Each layer has a convolutional operator followed by a ReLU activation function, where residual links and Batch Normalisation are excluded. A global average pool layer (GAP) brings the feature map resolution down to 1 x 1 before a fully-connected layer  \autocite{lin2021zen}.  

However, numerical overflow may occur for deep networks when directly calculating the $\Phi$-score because of gradient explosion without batch normalisation layers \autocite{lin2021zen}. Zen-Score addresses this problem by adding batch normalisation layers and re-scaling the $\Phi$-score by some constant. 

\subsection{Summary}

\Cref{tab:summary_zc} displays an overall view of the discussed zero-cost proxies, with the characteristics of each method's data dependence, data independence and type outlined. 

\begin{table}[htbp]
\centering
{\footnotesize
\caption{Summary of the implemented zero-cost proxies in the project}
\begin{tabular}{lccc}
\textbf{Zero-Cost Proxy} & \textbf{Data-dependent} & \textbf{Data-independent} & \textbf{Type} \\ \hline
\multicolumn{1}{l|}{\cellcolor{verylightgray}Epe-NAS} & \checkmark \cellcolor{verylightgray} & \cellcolor{verylightgray} & \cellcolor{verylightgray} Jacobian  \\
\multicolumn{1}{l|}{Fisher} & \checkmark  &  & Pruning-at-init  \\
\multicolumn{1}{l|}{\cellcolor{verylightgray}Flops} & \cellcolor{verylightgray} \checkmark & \cellcolor{verylightgray} & \cellcolor{verylightgray} Baseline \\
\multicolumn{1}{l|}{GradNorm} & \checkmark &  & Pruning-at-init  \\
\multicolumn{1}{l|}{\cellcolor{verylightgray}GradSign} & \cellcolor{verylightgray} \checkmark & \cellcolor{verylightgray} & \cellcolor{verylightgray} Gradient Sign \\
\multicolumn{1}{l|}{Grasp} & \checkmark  &  & Pruning-at-init  \\
\multicolumn{1}{l|}{\cellcolor{verylightgray}Jacov} & \cellcolor{verylightgray} \checkmark & \cellcolor{verylightgray} & \cellcolor{verylightgray} Jacobian \\
\multicolumn{1}{l|}{L2-norm} &  & \checkmark  & Baseline  \\
\multicolumn{1}{l|}{\cellcolor{verylightgray}Nwot} & \cellcolor{verylightgray} \checkmark & \cellcolor{verylightgray} & \cellcolor{verylightgray} Jacobian \\
\multicolumn{1}{l|}{Params} &  & \checkmark  & Baseline  \\
\multicolumn{1}{l|}{\cellcolor{verylightgray}Plain} & \cellcolor{verylightgray} \checkmark & \cellcolor{verylightgray} & \cellcolor{verylightgray} Baseline \\
\multicolumn{1}{l|}{Snip} & \checkmark  &  & Pruning-at-init \\
\multicolumn{1}{l|}{\cellcolor{verylightgray}Synflow} & \cellcolor{verylightgray} & \cellcolor{verylightgray} \checkmark & \cellcolor{verylightgray} Pruning-at-init \\
\multicolumn{1}{l|}{Zen} &  &  \checkmark & Piece. Lin.  \\
\end{tabular}
\label{tab:summary_zc}
}
\end{table}




