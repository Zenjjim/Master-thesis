\section{AutoML}\label{section:automl}
\glsreset{AutoML}

Fields such as computer vision, speech recognition, and natural language processing have seen significant progress, primarily due to the development and application of deep learning techniques in recent years. However, despite these advancements, designing optimal machine learning architectures remains complex and time-consuming for data scientists. \Gls{AutoML} is an area that seeks to automate this process.  

\subsection{Hyperparameter Optimisation}\label{subsection:hyperparameter-optimization}
Hyperparameter optimisation is a critical component of the \gls{AutoML} pipeline. Machine learning algorithms often contain hyperparameters that control their behaviour, and their optimal values are not learned directly from the training data. Instead, these parameters need to be set manually or searched for systematically. Traditional techniques for hyperparameter tuning include grid search, random search, and manual tuning. However, these approaches can be computationally expensive and inefficient.
\gls{AutoML} aims to streamline this process by employing advanced techniques such as Bayesian optimisation, evolutionary algorithms, and gradient-based methods. These methods can efficiently explore the hyperparameter space and identify suitable configurations, ultimately leading to improved model performance and reduced computational cost \autocite{bergstra2011algorithms, snoek2012practical}.

\subsection{Meta-learning}\label{subsection:meta-learning}
Meta-learning, also called "learning to learn," is another crucial component of \gls{AutoML}. The central idea is to utilise prior knowledge gained from solving multiple related tasks to improve the learning efficiency and generalisation ability on new tasks. This is achieved by learning a model or an algorithm that can adapt quickly to novel tasks with limited data.
In the context of \gls{AutoML}, meta-learning can be employed in various ways. One popular approach is to use meta-learning for transfer learning, wherein a pre-trained model is fine-tuned on a new task with limited available data \autocite{pan2010survey}. Another application of meta-learning is hyperparameter optimisation, where a meta-model can be trained to predict the performance of different configurations across various tasks. This knowledge can guide the search for optimal hyperparameters more efficiently \autocite{swersky2014freeze}.