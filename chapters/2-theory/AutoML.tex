\section{AutoML}\label{section:automl}

Over the past decade, machine learning has driven breakthroughs in numerous research areas and applications. Fields such as computer vision, speech recognition, and natural language processing have witnessed substantial progress, mainly attributed to the development of deep learning techniques. However, despite these advances, designing optimal machine learning architectures remains complex and time-consuming for data scientists. \Gls{AutoML} is an emerging area that seeks to automate this process, aiming to reduce the burden on researchers and practitioners alike. 

\subsection{Hyperparameter Optimisation}\label{subsection:hyperparameter-optimization}
Hyperparameter optimisation is a critical component of the AutoML pipeline. Machine learning algorithms often contain hyperparameters that control their behaviour, and their optimal values are not learned directly from the training data. Instead, these parameters need to be set manually or searched for systematically. Traditional techniques for hyperparameter tuning include grid search, random search, and manual tuning. However, these approaches can be computationally expensive and inefficient.
AutoML aims to streamline this process by employing advanced techniques such as Bayesian optimisation, evolutionary algorithms, and gradient-based methods. These methods can efficiently explore the hyperparameter space and identify suitable configurations, ultimately leading to improved model performance and reduced computational cost \autocite{bergstra2011algorithms, snoek2012practical}.

\subsection{Meta-learning}\label{subsection:meta-learning}
Meta-learning, also called "learning to learn," is another crucial component of AutoML. The central idea is to utilise prior knowledge gained from solving multiple related tasks to improve the learning efficiency and generalisation ability on new tasks. This is achieved by learning a model or an algorithm that can adapt quickly to novel tasks with limited data.
In the context of AutoML, meta-learning can be employed in various ways. One popular approach is to use meta-learning for transfer learning, wherein a pre-trained model is fine-tuned on a new task with limited available data \autocite{pan2010survey}. Another application of meta-learning is hyperparameter optimisation, where a meta-model can be trained to predict the performance of different configurations across various tasks. This knowledge can guide the search for optimal hyperparameters more efficiently \autocite{swersky2014freeze}.