
\section{Graph Convolutional Networks}

Graphs are ubiquitous structures representing complex systems, including social networks, biological networks, and transportation systems \autocite{scarselli2008graph}. They consist of nodes and edges, representing the many entities and relationships between them. Despite their widespread use, graphs pose unique challenges in machine learning due to their irregular structure, which makes it difficult to apply conventional learning techniques that rely on fixed-size inputs and Euclidean data representations \autocite{battaglia2018relational}.

\Gls{GNN} has emerged as a solution to these challenges, generalising deep learning techniques to graph-structured data \autocite{gori2005new}. Among \glspl{GNN}, \glspl{GCN} have become particularly popular owing to their ability to perform localised and efficient convolutions on graph data \autocite{DBLP:journals/corr/KipfW16}. \glspl{GCN} extend the concept of convolution from grid-like structures, such as images, to irregular graph structures, enabling the extraction of meaningful features from graph data while preserving their spatial relationships \autocite{bronstein2017geometric}. 

\subsection{Graph Convolutions}
The core of \glspl{GCN} lies in the graph convolution operation, which aims to aggregate local information from neighbouring nodes to generate a new representation for each node. The main idea is to perform a weighted sum of the feature vectors of a node and its neighbours, where the weights are determined by the edge weights or some measure of node similarity. This aggregation scheme allows \glspl{GCN} to learn node representations that effectively capture the local graph structure. The \cref{eq:kiph}, found in \autocite{DBLP:journals/corr/KipfW16}, shows how the graph convolution operation are mathematically described.

\begin{equation*} 
    \bm{H}^{l+1} = \sigma(\bm{\hat{D}}^{-\frac{1}{2}} \bm{A} \bm{\hat{D}}^{-\frac{1}{2}} \bm{H}^{l} \bm{W}^{l} )
    \label{eq:kiph}
\end{equation*}


The components in \cref{eq:kiph} are defined as follows:

$H^{(l+1)}$ stands for the updated node representations (features) at layer $(l+1)$, while $H^{(l)}$ indicates the node features at layer $l$. $H^{(0)}$ is the input feature matrix for the first layer. The graph's adjacency matrix, represented by $A$, is modified to incorporate self-connections. This is achieved by adding a diagonal matrix with ones to the original adjacency matrix $A$, ensuring that the node's features are considered during the aggregation process.

The degree matrix, denoted by $\hat{D}$, corresponds to the modified adjacency matrix $\hat{A}$. It is a diagonal matrix wherein each diagonal entry signifies the degree (number of connections) of the corresponding node. The learnable weight matrix at layer $l$ is represented by $W^{(l)}$ and is employed to linearly transform the node features at each layer.

Lastly, the activation function $\sigma$ introduces non-linearity to the model. Typical choices for activation functions are ReLU, sigmoid, and tanh.




