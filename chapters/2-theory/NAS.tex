\section{Neural Architecture Search (NAS)} \label{section:nas}
 \glsreset{NAS}
Most neural architectures are created by specialists, which is labour-intensive and susceptible to other weaknesses, including the need for an extensive expertise and the potential to introduce human bias. Subsequently, a way of automatically designing and developing such algorithms has been a research field for a couple of years. \gls{NAS} aims to automate the previously manual process of designing architectures \autocite{elsken2019neural}. Consequently, \gls{NAS} is a sub-field of \gls{AutoML}. 

Given a search space $F$, a training set $D_{\text{train}}$, validation set $D_{\text{valid}}$ and an evaluation metric $M$, a \gls{NAS} algorithm aims at finding an optimal architecture $f^* \in F$ with the best metric $M^*$ (such as validation accuracy) on the validation set $D_{\text{valid}}$. This can be written mathematically as shown in \cref{eq:nas}

\begin{equation}\label{eq:nas}
\begin{aligned}
    f^* &= \text{argmax}_{f \in F} M(f(\theta^*), D_{\text{valid}}), \\
    \theta^* &= \text{argmin}_{\theta} L(f(\theta), D_{\text{train}}),
\end{aligned}
\end{equation}

where $\theta^*$ is the learned parameters for the architecture $f$ and $L$ is the loss function \autocite{zhou2019auto}. 

\Cref{fig:nas_overview} illustrates the overall concept of \gls{NAS}. The search space gives the algorithm constraints regarding its development by defining a set of architectural choices the model might use. For example, a constraint might be different operations such as convolution, fully connected and pooling. One might argue that this is vital as selecting the search space can reduce the search's complexity, which is essential to produce an acceptable model \autocite{kyriakides2020introduction}.


\begin{figure}[h]
\begin{tikzpicture}
    \node (space) [rectangle, minimum width=3cm, minimum height=1.5cm, text centered, draw=black, align=center] {Search Space \\ $\mathcal{A}$};
    \node (strategy) [rectangle, minimum width=3cm, minimum height=1.5cm, text centered, draw=black, right=of space, xshift=1cm] {Search Strategy};
    \node (performance) [rectangle, minimum width=3cm, minimum height=1.5cm, text centered, draw=black, right=of strategy, xshift=1cm, align=center] {Performance\\ Estimation\\ Strategy};

    \draw[->, >=stealth, bend left] (strategy) to node[midway, above, align=center] {architecture \\ $A \in \mathcal{A}$} (performance);
    \draw[->, >=stealth, bend left] (performance) to node[midway, below, align=center] {performance \\ estimation of $A$} (strategy);

    \draw[thick, ->, >=stealth] (space) --(strategy);
\end{tikzpicture}
    \caption{An overview of the different methods in NAS \autocite{elsken2019neural} }
    \label{fig:nas_overview}
\end{figure}

After defining a search space for the given problem, the search strategy will specify how to analyse the search space and propose a set of candidate architectures. This introduces the exploration-exploitation trade-off, which indicates that selecting an appropriate optimisation technique is vital because we want to find a global optimum and ensure that the search space is sufficiently investigated \autocite{kyriakides2020introduction}. 

The framework must perform performance estimation for each candidate architecture to adjust the search strategy. The simplest solution is to train and validate the model. However, this might require many hours of training, which requires a lot of energy and has a high environmental cost. Another downside of training the architectures is that it will limit the number of architectures the search algorithm might discover. As a result, methods simplifying this phase have been undergoing heavy research \autocite{elsken2019neural}. 

Ultimately, the goal of NAS is to automatically discover the most optimal architecture regarding performance and processing power. 

\input{chapters/2-theory/NAS/weakness.tex}
\input{chapters/2-theory/NAS/search space}
\input{chapters/2-theory/NAS/search strategy}
\input{chapters/2-theory/NAS/performance estimation}