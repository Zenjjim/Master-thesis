\subsection{One-shot Graph Neural Architecture Search with Dynamic Search Space}

There are no apparent applications of traditional NAS methods for GNNs because GCNs naturally differ from CNNs. The main reason is that the search space of GNNs is much more significant because of the variety of GNNs' message-passing components. \cite{li2021one} propose a dynamic search space that maintains a subset of the significant search space and a set of importance weights for operation candidates in the subset as the architecture parameters. After each iteration, the subset is pruned by removing candidates with low-importance weights and expanding with new operations. This dynamic subset of operation candidates is tailored for each edge in the computation graph of the neural architecture, ensuring the diversity of operations in the final architecture. 

The paper demonstrates the effectiveness of this method through experiments on semi-supervised and supervised node classification tasks using citation networks, such as Cora, Citeseer, and Pubmed. The results show that the proposed method outperforms current state-of-the-art manually designed architectures and achieves competitive performance compared to existing GNN NAS approaches, with up to 10 times speedup.