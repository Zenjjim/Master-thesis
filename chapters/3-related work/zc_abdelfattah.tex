\subsection{Zero-Cost Proxies for Lightweight NAS}\label{abdelfattah}
\cite{abdelfattah2021zero} published a paper which did prominent research on the effect zero-cost proxies had on \gls{NAS}. The authors proposed seven different zero-cost proxies and investigated them in the context of \gls{NAS} on three different datasets; CIFAR-10 \autocite{Krizhevsky2009LearningML}, CIFAR-100 \autocite{Krizhevsky2009LearningML} and ImageNet16-120 \autocite{deng2009imagenet}. Spearman Rank was used to calculate the correlation between the validation accuracy and the provided score of each proxy. The validation accuracies were obtained using the NAS-Bench-201 benchmark \autocite{dong2020bench}. NAS-Bench-201 contains 15,625 models for the three different image classification datasets. The study demonstrated that the zero-cost proxies presented by the authors not only matched but also surpassed conventional methods in terms of Spearman Rank Correlation. The paper reported that \gls{Synflow} achieved a Spearman rank correlation of 0.82 on NAS-Bench-201, whereas \gls{EcoNAS} attained a correlation of 0.61. Later, the study was extended with three new benchmarks; NAS-Bench-101 \autocite{ying2019bench}, NAS-Bench-NLP \autocite{https://doi.org/10.48550/arxiv.2006.07116} and NAS-Bench-ASR \autocite{mehrotra2021bench}. The results showed that the \gls{Synflow} metric was the most consistent across the different benchmarks. 

Further, the paper showed how zero-cost proxies could be applied as a zero-cost warm-up, which means that the proxies are used at the start of the search to initialise the algorithm. Thus, the expensive training and evaluation process is removed. The crucial factor in the zero-cost warm-up is the number of models for which we calculate and utilise the zero-cost metric $(N)$. The advantage lies in that this value can often be much greater than the number of models we have the resources to train because $T << N$. The warm-up is performed on ageing evolution, reinforcement learning and a binary predictor. The results showed that even the moderated correlated zero-cost proxies significantly speed up the different search algorithms across the datasets. In addition, the algorithms can find good-performing architectures. 

Lastly, the authors showcased how the proxies can be used through a zero-cost move proposal. Contrary to zero-cost warm-up, the zero-cost move proposal concentrates on a local neighbourhood of each step to improve the selection of the following model for evaluation. 